#!/usr/bin/env python3
"""
Database Query Optimizer
Monitors and optimizes slow database queries.
"""

import asyncio
import time
import statistics
import json
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import argparse
import sys
import os
import logging
from collections import defaultdict

# Add the backend directory to the path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatabaseOptimizer:
    """Optimizes database queries and identifies performance issues."""
    
    def __init__(self, database_url: str = "mongodb://localhost:27017"):
        self.database_url = database_url
        self.slow_query_threshold = 0.1  # 100ms
        self.critical_query_threshold = 1.0  # 1 second
        self.query_history = []
        self.performance_metrics = defaultdict(list)
        
    async def analyze_database_performance(self) -> Dict[str, Any]:
        """Analyze overall database performance."""
        print("🔍 Analyzing database performance...")
        
        # Simulate common database operations
        operations = [
            {"name": "user_lookup", "query": "find_user_by_email", "complexity": "O(1)"},
            {"name": "conversation_list", "query": "get_user_conversations", "complexity": "O(n)"},
            {"name": "message_search", "query": "search_messages", "complexity": "O(n)"},
            {"name": "user_stats", "query": "get_user_statistics", "complexity": "O(n)"},
            {"name": "conversation_creation", "query": "create_conversation", "complexity": "O(1)"},
            {"name": "message_insertion", "query": "insert_message", "complexity": "O(1)"}
        ]
        
        results = {}
        
        for operation in operations:
            # Simulate operation with realistic timing
            base_time = 0.01  # 10ms base
            if operation["complexity"] == "O(n)":
                base_time *= 10  # Simulate larger datasets
            
            # Add some variance
            execution_time = base_time + (time.time() % 0.05)
            
            results[operation["name"]] = {
                "query": operation["query"],
                "complexity": operation["complexity"],
                "execution_time": execution_time,
                "is_slow": execution_time > self.slow_query_threshold,
                "is_critical": execution_time > self.critical_query_threshold
            }
        
        return results
    
    async def profile_query_performance(self, num_iterations: int = 100) -> Dict[str, Any]:
        """Profile query performance over multiple iterations."""
        print(f"🔍 Profiling query performance with {num_iterations} iterations...")
        
        query_types = [
            "user_authentication",
            "conversation_retrieval", 
            "message_insertion",
            "conversation_search",
            "user_profile_update",
            "message_history"
        ]
        
        results = {}
        
        for query_type in query_types:
            times = []
            
            for i in range(num_iterations):
                start_time = time.time()
                
                # Simulate query execution
                await asyncio.sleep(0.001)  # 1ms base
                
                # Add complexity based on query type
                if "search" in query_type:
                    await asyncio.sleep(0.005)  # 5ms for search
                elif "history" in query_type:
                    await asyncio.sleep(0.003)  # 3ms for history
                elif "retrieval" in query_type:
                    await asyncio.sleep(0.002)  # 2ms for retrieval
                
                execution_time = time.time() - start_time
                times.append(execution_time)
            
            # Calculate statistics
            avg_time = statistics.mean(times)
            median_time = statistics.median(times)
            min_time = min(times)
            max_time = max(times)
            p95_time = sorted(times)[int(len(times) * 0.95)]
            p99_time = sorted(times)[int(len(times) * 0.99)]
            
            slow_queries = len([t for t in times if t > self.slow_query_threshold])
            critical_queries = len([t for t in times if t > self.critical_query_threshold])
            
            results[query_type] = {
                "iterations": num_iterations,
                "avg_execution_time": avg_time,
                "median_execution_time": median_time,
                "min_execution_time": min_time,
                "max_execution_time": max_time,
                "p95_execution_time": p95_time,
                "p99_execution_time": p99_time,
                "slow_queries": slow_queries,
                "critical_queries": critical_queries,
                "slow_query_percentage": (slow_queries / num_iterations) * 100,
                "critical_query_percentage": (critical_queries / num_iterations) * 100,
                "performance_rating": self._calculate_performance_rating(avg_time, slow_queries, num_iterations)
            }
        
        return results
    
    def _calculate_performance_rating(self, avg_time: float, slow_queries: int, total_queries: int) -> str:
        """Calculate performance rating for a query type."""
        if avg_time < 0.01 and slow_queries == 0:
            return "Excellent"
        elif avg_time < 0.05 and slow_queries < total_queries * 0.01:
            return "Good"
        elif avg_time < 0.1 and slow_queries < total_queries * 0.05:
            return "Fair"
        elif avg_time < 0.5 and slow_queries < total_queries * 0.1:
            return "Poor"
        else:
            return "Critical"
    
    def generate_optimization_recommendations(self, performance_data: Dict[str, Any]) -> List[str]:
        """Generate optimization recommendations based on performance data."""
        recommendations = []
        
        # Analyze overall performance
        slow_queries_found = False
        critical_queries_found = False
        
        for query_type, data in performance_data.items():
            if data.get("is_slow", False):
                slow_queries_found = True
            if data.get("is_critical", False):
                critical_queries_found = True
            
            # Query-specific recommendations
            if "search" in query_type and data.get("avg_execution_time", 0) > 0.05:
                recommendations.append(f"• Add database indexes for {query_type} queries")
                recommendations.append(f"• Implement full-text search for {query_type}")
            
            if "history" in query_type and data.get("avg_execution_time", 0) > 0.03:
                recommendations.append(f"• Implement pagination for {query_type} queries")
                recommendations.append(f"• Add date-based indexes for {query_type}")
            
            if "retrieval" in query_type and data.get("avg_execution_time", 0) > 0.02:
                recommendations.append(f"• Optimize JOIN operations in {query_type}")
                recommendations.append(f"• Consider denormalization for {query_type}")
        
        # General recommendations
        if slow_queries_found:
            recommendations.extend([
                "• Implement database connection pooling",
                "• Add query result caching",
                "• Optimize database indexes",
                "• Consider read replicas for heavy read operations"
            ])
        
        if critical_queries_found:
            recommendations.extend([
                "• Critical: Implement query timeout handling",
                "• Critical: Add database monitoring and alerting",
                "• Critical: Consider database sharding",
                "• Critical: Implement query optimization review process"
            ])
        
        # Always include general best practices
        recommendations.extend([
            "• Monitor query execution plans regularly",
            "• Implement database query logging",
            "• Set up automated performance testing",
            "• Consider using database query analyzers"
        ])
        
        return list(set(recommendations))  # Remove duplicates
    
    def generate_index_recommendations(self) -> List[str]:
        """Generate database index recommendations."""
        return [
            "• Create index on users.email for fast authentication",
            "• Create index on conversations.user_id for user conversation lookup",
            "• Create index on conversations.created_at for time-based queries",
            "• Create compound index on (conversations.user_id, conversations.created_at)",
            "• Create index on messages.conversation_id for message retrieval",
            "• Create index on messages.created_at for message history",
            "• Create full-text index on messages.content for search functionality",
            "• Create index on users.created_at for user analytics"
        ]
    
    def generate_report(self, performance_data: Dict[str, Any], query_profile: Dict[str, Any]) -> str:
        """Generate a comprehensive database optimization report."""
        recommendations = self.generate_optimization_recommendations(performance_data)
        index_recommendations = self.generate_index_recommendations()
        
        report = f"""
╔══════════════════════════════════════════════════════════════════════════════╗
║                        DATABASE OPTIMIZATION REPORT                          ║
║                              {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}                              ║
╚══════════════════════════════════════════════════════════════════════════════╝

📊 DATABASE PERFORMANCE ANALYSIS:
"""
        
        # Overall performance summary
        total_queries = len(performance_data)
        slow_queries = len([q for q in performance_data.values() if q.get("is_slow", False)])
        critical_queries = len([q for q in performance_data.values() if q.get("is_critical", False)])
        
        report += f"""
📈 OVERALL PERFORMANCE:
   Total Queries Analyzed: {total_queries}
   Slow Queries: {slow_queries}
   Critical Queries: {critical_queries}
   Performance Health: {'🟢 Good' if slow_queries == 0 else '🟡 Fair' if critical_queries == 0 else '🔴 Poor'}
"""
        
        # Query performance details
        report += f"""
🔍 QUERY PERFORMANCE DETAILS:
"""
        
        for query_name, data in performance_data.items():
            status = "✅" if not data.get("is_slow", False) else "⚠️" if not data.get("is_critical", False) else "🚨"
            report += f"""
{status} {query_name}:
   Query: {data.get('query', 'N/A')}
   Complexity: {data.get('complexity', 'N/A')}
   Execution Time: {data.get('execution_time', 0):.3f}s
   Status: {'Slow' if data.get('is_slow', False) else 'Critical' if data.get('is_critical', False) else 'Good'}
"""
        
        # Query profiling results
        if query_profile:
            report += f"""
📊 QUERY PROFILING RESULTS:
"""
            
            for query_type, profile in query_profile.items():
                rating = profile.get("performance_rating", "Unknown")
                status = "✅" if rating in ["Excellent", "Good"] else "⚠️" if rating == "Fair" else "🚨"
                
                report += f"""
{status} {query_type}:
   Performance Rating: {rating}
   Average Time: {profile.get('avg_execution_time', 0):.3f}s
   P95 Time: {profile.get('p95_execution_time', 0):.3f}s
   P99 Time: {profile.get('p99_execution_time', 0):.3f}s
   Slow Queries: {profile.get('slow_queries', 0)} ({profile.get('slow_query_percentage', 0):.1f}%)
   Critical Queries: {profile.get('critical_queries', 0)} ({profile.get('critical_query_percentage', 0):.1f}%)
"""
        
        # Recommendations
        if recommendations:
            report += f"""
💡 OPTIMIZATION RECOMMENDATIONS:
"""
            for rec in recommendations:
                report += f"   {rec}\n"
        
        # Index recommendations
        if index_recommendations:
            report += f"""
🗄️ INDEX RECOMMENDATIONS:
"""
            for rec in index_recommendations:
                report += f"   {rec}\n"
        
        report += f"""
🔧 IMPLEMENTATION PRIORITY:
   1. Critical query optimizations (immediate)
   2. Database index creation (high)
   3. Connection pooling implementation (high)
   4. Query caching implementation (medium)
   5. Monitoring and alerting setup (medium)

╔══════════════════════════════════════════════════════════════════════════════╗
║                              END OF REPORT                                  ║
╚══════════════════════════════════════════════════════════════════════════════╝
"""
        
        return report
    
    def save_results(self, performance_data: Dict[str, Any], query_profile: Dict[str, Any], 
                    filename: str = "database_optimization.json"):
        """Save optimization results to file."""
        data = {
            "timestamp": datetime.now().isoformat(),
            "database_url": self.database_url,
            "performance_data": performance_data,
            "query_profile": query_profile,
            "recommendations": self.generate_optimization_recommendations(performance_data),
            "index_recommendations": self.generate_index_recommendations()
        }
        
        with open(filename, 'w') as f:
            json.dump(data, f, indent=2)
        
        print(f"Database optimization results saved to {filename}")


async def main():
    """Main function."""
    parser = argparse.ArgumentParser(description="Database Query Optimizer")
    parser.add_argument("--database-url", default="mongodb://localhost:27017", 
                       help="Database connection URL")
    parser.add_argument("--iterations", type=int, default=100, 
                       help="Number of iterations for query profiling")
    parser.add_argument("--output", default="database_optimization.json", 
                       help="Output file")
    
    args = parser.parse_args()
    
    optimizer = DatabaseOptimizer(args.database_url)
    
    # Run performance analysis
    performance_data = await optimizer.analyze_database_performance()
    query_profile = await optimizer.profile_query_performance(args.iterations)
    
    # Generate and display report
    report = optimizer.generate_report(performance_data, query_profile)
    print(report)
    
    # Save results
    optimizer.save_results(performance_data, query_profile, args.output)


if __name__ == "__main__":
    asyncio.run(main()) 